---
title: "Lab1"
author: "Pradip Shrestha"
date: "11/5/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### What are the benefits and challenges of an open data science approach? Give an example based on this week's reading. 

<font size = "3.5"> In recent years, big data has burst from controlled, automated, and volunteered sources, promising a data deluge attributable to rich, detailed, interconnected, timely, and low-cost characters. The advantages of the open data science approach include the ability to transition from data-scarce to data-rich studies, static snapshots to dynamic unfolding, coarse aggregations to high resolutions; relatively simple hypotheses and models to more complex, sophisticated simulations and theories; and relatively simple hypotheses and models to more complex, sophisticated simulations and theories. It has the potential for a paradigm change in social sciences applications, in addition to its motivation for human geography. For instance, President Barack Obama's campaign team compiled data from hundreds of large-scale randomized polling experiments, tracking website cookies, and a variety of sources, including registration data, census, and other government data, commercial data aggregates, credit rating agencies, cable television companies, and social media sites, during the 2008 and 2012 elections. If made publicly available for analysis, it would offer a rich insight into the social, political, economic, and spatial worlds. 

Big data, on the other hand, brings with it a slew of issues. New ways for processing and evaluating various types of datasheets containing millions or billions of observations collected on a dynamic basis are required at the most fundamental level. Traditional methods and techniques for handling and analyzing both structured and unstructured data may be limited. There's also some irony in the fact that, despite the deluge of data, access to it is extremely restricted. This is because big data is primarily generated by privately held businesses and the government. Another concern is that, if access is granted, working with such data creates several ethical and security issues. Although big data, in general, captures the face value from freely articulated data, it struggles to capture complicated emotions, values, views, and opinions, as well as the many different, contextual, rational, and irrational ways in which individuals interact and make sense of the world. </font>

Reference: 
Kitchin, R. (2013). Big data and human geography: Opportunities, Challenges and Risks. *Dialogues in Human Geography*, 3(3), 262-267.

#### Knit a markdown document that demonstrates an analysis of this or other data (include: text explaining your analysis, figures and geovisualizations) 

```{r}
## Dependencies
library(sf)
library(raster)
library(rgdal)
library(ggplot2)
library(dplyr)
library(geosphere)
library(osmdata)
library(ggsn)
library(ggspatial)
library(viridis)
```


```{r}
# Set working directory
setwd("/cloud/project/Lab 1")
```
Social media platform is a powerful communicator and source of information in form of video, blogs, photographs and commentary. By nature this data is big, messy and noised and would not make any sense but with some cleaning (wrangling techniques) this be of immense importance to support further analysis in people behavior and preferences. This lab is based on the location-based analysis of geo-tagged social media post data of Open Spaces and Mountain Parks in Boulder Colorado. Among the factors influencing hiking behavior, hiking difficulty and distance to street may have confounding influence on the behavioral patterns (taking photographs).

As such, I will explore this spatial attribute related to social media posts. Particularly, the analysis will be based on assumption that higher the hiking difficulty lesser will be the social media posts. This analysis will help to understand the effect of distance as a proxy to human behaviour in the Mountain park. 

We begins with setting up the work directory and importing the data as a `sf` spatial dataset. Here we transform the coordinates system of the dataset to NAD27/Colorado North. 

```{r}
# Import dataset
boulder <- st_read("BoulderSocialMedia.shp")

#Transform CRS
boulder = st_transform(boulder, 26753)
```
To explore the data, we use ```summary``` and group attributes using ```by``` command. We exclude few column that are irrelevant to summary statistics.   

```{r}
boulder %>% 
  select (-(id | DB | extent | geometry)) %>% 
  summary()

by (boulder$PT_Elev, boulder$DB, summary)
```
The descriptive statistics of the variable shows the mean, median, quantile and range value of each variables in the datasheet.The data contains 55519 features and 12 fields point data and has associated projection. Although the group exhibits similar range for elevation, among them highest range was recorded for Flickr whereas mean elevation was highest for point in the park.  

We first examine the spatial extent of social media post (Flickr and Panramio) against point in the park using ```ggplot```. In ggplot, we pass ```mutate``` function to add new variables that satisfy the given condition that either the data belongs to Flickr or Panramio and then plot the result.   

```{r, fig.width = 10}
boulder %>%
  mutate(DB = ifelse(DB == 'Flickr' | DB =='Pano', TRUE, FALSE))%>% 
  ggplot() +
  geom_sf(aes(color=DB),
          fill = NA, alpha = .1)  +  
  theme_bw()+
  labs(y="Latitude", 
       x="Longitude", 
       title="Social Media Posts", 
       subtitle = "From Flickr or Panramio",
       caption = "Source: Lab 1_data")
```

Here spatial data that belongs either to Flickr or Panramio are shows as a TRUE value whereas other are recorded FALSE. From graphs, its obvious that datasheet is dominated with point in the park photographs whereas social media data are comparatively scarce. The datasheet was categorized according to DB type using ```filter``` command. A hiking difficulty based on a arbitrary numerical rating using the following formula:

$$ Difficultyscale = \sqrt{Elevation * 2 * distance}$$

```{r}
social_data <- boulder[boulder$DB == 'Pano' | boulder$DB == 'Flickr', ]
social_data$hiking_scale <- sqrt(social_data$PT_Elev * social_data$Trails_dis * 2)
summary(social_data$hiking_scale)
```

After isolating only social media posts data, the hiking scale was catagorized based on the quantile classes at an incremental order ranging from easiest to very strenuous. This was plotted against the distance to streets and parking lots using ```box plot```.      

```{r}
hike_class<- cut(social_data$hiking_scale, br=c(-1, 200, 400, 700, 2800), 
                 labels= c("Easiest","Moderate", "Strenuous", "Very Strenuous"))

#plot
social_data %>% 
  ggplot(aes(x=hike_class, y=Street_dis)) + 
  geom_boxplot(aes(color = hike_class)) + 
  theme_classic() +
  labs(subtitle="Hiking difficulty Vs Street & Parking location", 
       y="Distance to Street and Parking", 
       x="Hike Difficulty Class", 
       title="Boxplot", 
       caption = "Source: Lab 1_data") + theme(legend.position = "none")
```

From boxplot, it is obvious that closer the distance to the street and parking, lower is the hiking difficulty. To make the comparison between the variables ```scatter plot``` function was applied within ```ggplot```. Following, linearity of the variables is measure to ascertain the positive or a negative relation. For this, ```geom_smooth``` was applied to draws a smoothing line (based on linear model). 

```{r}
plot1 <- ggplot(social_data, aes(x=hiking_scale, y=Street_dis)) + geom_point(aes(col=DB)) + 
  geom_smooth(method="lm", se=F, color="blue") + 
  xlim(c(0, 2500)) + 
  ylim(c(0, 2500)) + 
  labs(subtitle="Relation between Distance to Street & Parking to Hiking scale", 
       y="Street & Parking distance (feet)", 
       x="Hiking Scale", 
       title="Scatterplot", 
       caption = "Source: Lab 1_data") + theme_classic()
plot(plot1)
```

From plot 1, it can be inferred that distance to street & parking shows some linearity with hiking scale of the location for social post data. In analyzing the linearity, ```linearMod```, the p-Values are well below the 0.05 threshold, so we can conclude our model is indeed statistically significant.This shows that people were inclined to take more pictures and posting them in social sites when hiking was relatively easy and close to streets and parking.  

```{r}
linearMod1 <- lm(Street_dis ~ hiking_scale, data=social_data)  # built linear regression model
summary(linearMod1)
```

Finally, the difficulty scale was plotted with a ```OSM basemap``` using appropriate map elements (north arrow, scale, title). 

```{r, fig.width = 10}
extent<- data.frame(x = c(-105.33, -105.23),   y = c(39.93, 40.04))
plot_extent <- ggplot(extent) +
  theme_bw() + annotation_map_tile(type = "https://tiles.wmflabs.org/osm-no-labels/${z}/${x}/${y}.png") + 
  layer_spatial(social_data, aes(col = hiking_scale), fill = NA, alpha = .1) + 
  scale_colour_gradientn(colors = viridis(n=256, option = "G")) + geom_spatial_point(aes(x, y), crs = 4326) + annotation_scale(location = "br", bar_cols = c("black", "white"), style = "bar") +
  annotation_north_arrow(which_north = "grid") + labs(y="Latitude", # adding axis title
                                                      x="Longitude",
                                                      subtitle="With Hiking Difficulty Scale",
                                                      title="Open Street Map of Mountain Park",
                                                      caption = "Source: Lab 1_data") + 
  labs(color ="Difficulty scale")+ # formatting legend
  guides(fill = guide_legend(reverse=TRUE)) 
plot_extent
```

#### Bonus: Include a screen grab of the history of your git commits. What is your strategy for using git? 

<font size = "3.5"> I plan to use GIT to track changes I would make into a project that I am working on, or collaborative work such as Master's Project with my group. It helps to me to keep update with the changes made over time. Git, an open source endeavor, helps in sharing our work with others while allowing us to view changes and specific versions of those files later on, if needed.Another use of GIT can be uploading the lab assignments, as a backup repository, in case there is some technical glitches or is damaged. </font>  

Below is an image of my GIT commits.

!["Pradip Git Commits"](download.png)

